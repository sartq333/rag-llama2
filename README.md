# RAG using llama2 from ollama and langchain

This repository provides the gentle introduction to RAG and how the LLMs can be provided with a context so that their response is more relevant and in accordance with the user's query.

If you want to use it directly, here's the colab link of the notebook: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1HZP7NYnFfUV3vQ_-VKQeyBaACBzMLDOO?usp=sharing)

References: 

https://python.langchain.com/docs/integrations/chat/ollama/

https://github.com/svpino/llm/blob/main/local.ipynb

https://ollama.com/search

https://www.youtube.com/watch?v=rhZgXNdhWDY 
